Flow of the application: 

Sensor is moved around - and wherever the VOC spikes above a given threshold value - The sensor beeps and the user is prompted to take a photograph(s). [1]
The photo is uploaded on the server. 
The value of the VOC content from the server is also sent to the server 
Both the photo and label (VOC content) are added to a separate database -- This will be the database that our model will be trained on.
If this is the Naive User activity of the Android App then: 
The App will now ask the user the following things 
How many windows does this room have ? [2] (n)
Is there a source of mechanical ventilation in the room ? If yes, Specify how many. (m)
These responses will be sent to the server 
Weather API calls will be made to obtain wind speed and wind direction (w,d)
All these will be used to output a list of recommendations (Python program that will run on the server along with the model inference - Algorithm specified below)
These recommendations + Label of the image will be sent to the Android App and displayed in the form of a report.
 b. The Server will also run the inference on the image that has been uploaded and       that will be returned as the label.     
 


Recommender System: 

Since the object recommender is not a usable option (the lack of an appropriate dataset) - We can use the following naive recommender: 

Let the number of windows be = n 
Let the percentage of sunlight in a room be = s (in percent)
Let the VOC content recorded by the sensor be = v 
Let the source(s) of mechanical ventilation in the room be =  m
Let the Wind speed be = w (km/h)
Let the Wind direction be = d (degrees)
Let the age of the article = a (years)

Data Structure : Decision Tree 

Algo:
recommend(char ch)
{
	if(ch==’a’):
	{
		check n; 
		if(n!=0):  //if the rooms has some windows that can be used for natural ventilation	
		{
			check if(w*d > air_dispersion_threshold_value) 
{
/*only when the air dispersion  (w*d) outside is high enough, should we suggest natural ventilation, otherwise, the outside Air Quality + VOC content could negatively affect the VOC conc inside the house (eg: for a user in New Delhi) */

	if (v  > 50)
		t = 30 minutes;  //t = how many minutes the user should allow ventilation
	if( v > 100)
		t = 60 minutes;
	...
  
	cout<<”Open the windows for”<<t<<”minutes”;
}
else
{
	//(w*d) is too low or if there are no windows 
	if(m ==1 )
		t = 60 minutes;
	if(m ==2)
		t= 30 minutes;
	…
	cout<<”Use mechanical ventilation for<<”t<<”minutes”;

}

}

}
if(ch==’b’)
{
	//if it older than a year - most prob furniture components or varnishes will be a source
	//Will add recommendations according to https://blog.paleohacks.com/toxins-in-furniture/
}
} 

if (v>threshold_value age<1 year): // high VOC content with new items 
	recommend(a)
/*Mostly happen with dry cleaning, cleaning supplies, etc -- stuff that can be moved    around easily*/
else
	//high VOC content with older items 
	recommend(b)
	
}
 





Can Check if we can implement: 
Can a buzzer module be added to the handheld device so that it beeps as the VOC content in a region is increasing : 
VOC >=50:
	buzzer_frequency = 2 bps (bps = beeps per second)
VOC >=100:
	buzzer_frequency = 4 bps (bps = beeps per second)
VOC >=150:
	buzzer_frequency = 8 bps (bps = beeps per second)

Can a module that estimates the amount of sunlight in a room be added to the  handheld device because with the rover it will be very inconvenient for the user to input the number of windows in each and every area where there is a VOC spike (for a commercial place)

# Feature Research 
General Need of VOC Estimation:
https://iaqscience.lbl.gov/voc-cleaning

##SMELL 
https://branchbasics.com/blog/vocs-and-how-they-affect-your-health/

Recommender - 
https://branchbasics.com/blog/nontoxic-air-fresheners-remove-odors-naturally/

##VENTILATION 
Mechanical ventilation + Wind speed + Wind Direction : Only when the wind speed outside is high enough, we will suggest opening windows : Only suggest opening them when the moisture outside is high 
##OUTDOOR EFFECTS
https://ansn.iaea.org/Common/Topics/OpenTopic.aspx?ID=13012
Guassian Plume Model 

##SUNLIGHT 
https://branchbasics.com/blog/sunning-how-to-take-advantage-of-the-summer-sun-by-outgassing/

Help in reducing the conc of VOC 
along with proper ventilation 

##Time 
How long have you had this article --  days/years

##THRESHOLD VALUES OF THE VOC 
https://foobot.io/guides/threshold-limit-values-volatile-organic-compounds.php

##Datasets 

MOST IMP - https://www.kaggle.com/c/imaterialist-challenge-furniture-2018/data
http://homepages.inf.ed.ac.uk/rbf/CVonline/Imagedbase.htm#object

MODEL ARCH: 

Sources : https://towardsdatascience.com/transfer-learning-from-pre-trained-models-f2393f124751
		  https://towardsdatascience.com/a-comprehensive-hands-on-guide-to-transfer-learning-with-real-world-applications-in-deep-learning-212bf3b2f27a
		  https://keras.io/applications/


Architecture choice : https://medium.com/zylapp/review-of-deep-learning-algorithms-for-object-detection-c1f3d437b852

MORE IMP : https://cv-tricks.com/object-detection/faster-r-cnn-yolo-ssd/

Architecture Rationale: 

Metric 
The object detection challenge is, at the same time, a regression and a classification task. First of all, to assess the spatial precision we need to remove the boxes with low confidence (usually, the model outputs many more boxes than actual objects). Then, we use the Intersection over Union (IoU) area, a value between 0 and 1. It corresponds to the overlapping area between the predicted box and the ground-truth box. The higher the IoU, the better the predicted location of the box for a given object. Usually, we keep all bounding box candidates with an IoU greater than some threshold.

In binary classification, the Average Precision (AP) metric is a summary of the precision-recall curve, details are provided here. The commonly used metric used for object detection challenges is called the mean Average Precision (mAP). It is simply the mean of the Average Precisions computed over all the classes of the challenge. The mAP metric avoids to have extreme specialization in few classes and thus weak performances in others.


RCNN - Region based -- Localise the object along the same regional similarity 


The R-CNN model (R. Girshick et al., 2014) combines the selective search method to detect region proposals and deep learning to find out the object in these regions. Each region proposal is resized to match the input of a CNN from which we extract a 4096-dimension vector of features. The features vector is fed into multiple classifiers to produce probabilities to belong to each class. Each one of these classes has a SVM classifier trained to infer a probability to detect this object for a given vector of features. This vector also feeds a linear regressor to adapt the shapes of the bounding box for a region proposal and thus reduce localization errors.


FAST RCNN 

A main CNN with multiple convolutional layers is taking the entire image as input instead of using a CNN for each region proposals (R-CNN). Region of Interests (RoIs) are detected with the selective search method applied on the produced feature maps. Formally, the feature maps size is reduced using a RoI pooling layer to get valid Region of Interests with fixed heigh and width as hyperparameters. Each RoI layer feeds fully-connected layers¹ creating a features vector. The vector is used to predict the observed object with a softmax classifier and to adapt bounding box localizations with a linear regressor.

FASTER RCNN 

Region proposals detected with the selective search method were still necessary in the previous model, which is computationally expensive. S. Ren and al. (2016) have introduced Region Proposal Network (RPN) to directly generate region proposals, predict bounding boxes and detect objects. The Faster Region-based Convolutional Network (Faster R-CNN) is a combination between the RPN and the Fast R-CNN model.


Faster R-CNN uses RPN to avoid the selective search method, it accelerates the training and testing processes, and improve the performances. The RPN uses a pre-trained model over the ImageNet dataset for classification and it is fine-tuned on the PASCAL VOC dataset. Then the generated region proposals with anchor boxes are used to train the Fast R-CNN. This process is iterative.

RFCN -- Fully Convolutional :: used for both detection as well as localisation of the position of the detected object 

A ResNet-101 model takes the initial image as input. The last layer outputs feature maps, each one is specialized in the detection of a category at some location. 


YOLO 
Initially, the model takes an image as input. It divides it into an SxS grid. Each cell of this grid predicts B bounding boxes with a confidence score. This confidence is simply the probability to detect the object multiply by the IoU between the predicted and the ground truth boxes.

Will be using YOLO because of the Speed and computational inexpense 

-- Graphic config: Intel HD Graphics 620.

SSD - Single shot detector 

The SSD model uses extra feature layers from different feature maps of the network in order to increase the number of relevant bounding boxes. 

IMAGE NET : 500 IMAGES on average 

rocking chair 
folding chair 
barber chair 
dining table 
pool table 
day bed, couch 
hair spray 

PASCAL VOC : https://pjreddie.com/media/files/VOC2012_doc.pdf : 300 max 
bottle
chair
dining table 
sofa

COCO : https://tech.amikelive.com/node-718/what-object-categories-labels-are-in-coco-dataset/ 

door
desk 
chair		
couch	
bed	
dining table	
window


